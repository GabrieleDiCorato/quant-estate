{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Pipeline 2: Listings\n",
    "\n",
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Find the project root\n",
    "project_root = Path().cwd().parent\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "# Add project root to Python path (not just sources)\n",
    "sys.path.insert(0, str(project_root))\n",
    "print(f\"Added to Python path: {project_root}\")\n",
    "\n",
    "# Set environment variables\n",
    "\n",
    "os.environ[\"QE_ENV\"] = \"dev\"\n",
    "os.environ[\"QE_CONF_FOLDER\"] = \"sources/resources\"\n",
    "print(f\"Added environment variables: QE_ENV={os.environ['QE_ENV']}, QE_CONF_FOLDER={os.environ['QE_CONF_FOLDER']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sources.datamodel.listing_details import ListingDetails\n",
    "from sources.datamodel.listing_id import ListingId\n",
    "from sources.logging import logging_utils\n",
    "from sources.storage.abstract_storage import Storage\n",
    "from sources.scrapers.immobiliare.scraper_listing import ImmobiliareListingScraper\n",
    "from sources.config.config_manager import ConfigManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_utils.setup_logging(config_path=\"sources/resources/logging.yaml\")\n",
    "logger = logging_utils.get_logger(__name__)\n",
    "\n",
    "config_manager = ConfigManager()\n",
    "config_manager.invalidate_caches()\n",
    "\n",
    "storage_settings = config_manager.get_storage_config()\n",
    "scraper_settings = config_manager.get_scraper_listing_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage: Storage = Storage.create_storage(\n",
    "    data_type=ListingDetails, \n",
    "    config=storage_settings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Extract ListingIds from MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from contextlib import contextmanager\n",
    "\n",
    "from sources.config.model.storage_settings import MongoStorageSettings\n",
    "from sources.storage.mongo_storage import MongoDBStorage\n",
    "\n",
    "\n",
    "# Get MongoDB configuration from storage settings\n",
    "mongo_config: MongoStorageSettings = storage_settings.mongodb_settings  # This should be a MongoStorageSettings instance\n",
    "\n",
    "\n",
    "# Connect to MongoDB using the same configuration as the storage\n",
    "@contextmanager\n",
    "def get_mongo_client():\n",
    "\n",
    "    \"\"\"Context manager for MongoDB client with proper resource cleanup.\"\"\"\n",
    "\n",
    "    client = MongoClient(mongo_config.connection_string.get_secret_value())\n",
    "\n",
    "    try:\n",
    "        yield client\n",
    "\n",
    "    finally:\n",
    "        client.close()\n",
    "\n",
    "\n",
    "# Query for ListingIds that don't have corresponding ListingDetails using aggregation\n",
    "\n",
    "batch_size = 5\n",
    "\n",
    "\n",
    "with get_mongo_client() as client:\n",
    "\n",
    "    db = client[mongo_config.database]\n",
    "    ids_collection = db[mongo_config.collection_ids]\n",
    "\n",
    "    # Use aggregation pipeline with $lookup (left outer join) to find unprocessed IDs\n",
    "    pipeline = [\n",
    "        {\n",
    "            \"$lookup\": {\n",
    "                \"from\": mongo_config.collection_listings,  # Join with listings collection\n",
    "\n",
    "                \"localField\": \"id\",  # Field from ids collection\n",
    "                \"foreignField\": \"id\",  # Field from listings collection\n",
    "\n",
    "                \"as\": \"listing_details\",  # Output array field\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "\n",
    "            \"$match\": {\n",
    "\n",
    "                \"listing_details\": {\n",
    "                    \"$size\": 0\n",
    "                }  # Filter where no matching listing details found\n",
    "\n",
    "            }\n",
    "\n",
    "        },\n",
    "        {\n",
    "            \"$sample\": {\"size\": batch_size}  # Randomly sample from matching documents\n",
    "        },\n",
    "        {\n",
    "            \"$project\": {\n",
    "\n",
    "                \"listing_details\": 0  # Remove the empty listing_details array from output\n",
    "            }\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Execute aggregation pipeline\n",
    "    unprocessed_docs = list(ids_collection.aggregate(pipeline))\n",
    "\n",
    "\n",
    "    # Convert documents back to ListingId objects\n",
    "\n",
    "    listing_ids = [ListingId.from_dict(doc) for doc in unprocessed_docs]\n",
    "\n",
    "\n",
    "    print(f\"Found {len(listing_ids)} ListingIds without corresponding ListingDetails\")\n",
    "\n",
    "\n",
    "listing_ids[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Scraping Listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures as cf\n",
    "import time\n",
    "\n",
    "# Stagger the start times to be respectful to the server\n",
    "max_workers = 10 # Conservative limit for immobiliare.it\n",
    "stagger_delay = 35  # Seconds between starting each scraper\n",
    "\n",
    "def run_scraper_with_delay(\n",
    "    listing_id: ListingId, \n",
    "    delay,\n",
    "):\n",
    "    \"\"\"Run scraper with staggered start to avoid overwhelming the server\"\"\"\n",
    "    time.sleep(random.uniform(0, delay))\n",
    "    scraper = ImmobiliareListingScraper(storage, settings=scraper_settings, listing_id=listing_id)\n",
    "    return scraper.scrape()\n",
    "\n",
    "\n",
    "with cf.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = []\n",
    "    for i, id in enumerate(listing_ids):\n",
    "        future = executor.submit(run_scraper_with_delay, id, stagger_delay)\n",
    "        futures.append(future)\n",
    "\n",
    "    for future in cf.as_completed(futures):\n",
    "        try:\n",
    "            result = future.result()\n",
    "            logger.info(\"Scraper completed\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Scraper failed: {e}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
